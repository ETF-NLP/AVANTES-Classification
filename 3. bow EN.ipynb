{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "from nltk.tokenize import word_tokenize, WhitespaceTokenizer\n",
    "import re\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_validate\n",
    "from sklearn.pipeline import make_pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"./data/corpus.csv\"\n",
    "corpus = pd.read_csv(\"./data/corpus.csv\", dtype=\"string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EN data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = \"EN\"\n",
    "data = corpus[corpus.NaturalLanguageID == language]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data[[\"Comment\", \"y8\", \"y6\", \"y2\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_tokens(name, data, lang=language):\n",
    "    output_file_name = f\"data/tokens/{lang}_{name}.csv\"\n",
    "    \n",
    "    # X_temp = X.apply(lambda tokens: \" \".join(tokens))\n",
    "    \n",
    "    print(f\"Writing {name} {data.shape} to {output_file_name}.\")\n",
    "    data.to_csv(output_file_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_results(result_file, score_name, score_value):\n",
    "    pd.DataFrame(\n",
    "        {\"score_name\": [score_name],\n",
    "        \"score_value\": [score_value]}\n",
    "    ).to_csv(result_file, mode=\"a\", decimal=\",\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_score_name(score_name, model_name, num_classes):\n",
    "    return f\"{score_name}-{model_name}-{num_classes}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data, x_column_name, y_column_name, result_file, score_name, model_name, estimator, hyper_params):\n",
    "    print(estimator)\n",
    "    \n",
    "    X = data[x_column_name]\n",
    "    y = data[y_column_name]\n",
    "\n",
    "    full_score_name = make_score_name(score_name, model_name, y.nunique())\n",
    "    print(f\"--------Evaluating {full_score_name} --------\")\n",
    "    gs_estimator = GridSearchCV(\n",
    "        estimator, hyper_params, scoring=\"f1_macro\", cv=StratifiedKFold(n_splits=10, shuffle=True, random_state=42), verbose=0, n_jobs=-1)\n",
    "\n",
    "    scores = cross_validate(\n",
    "        gs_estimator, X, y, scoring=\"f1_macro\", cv=StratifiedKFold(n_splits=10, shuffle=True, random_state=42), verbose=0, n_jobs=-1)\n",
    "    mean_score = np.mean(scores[\"test_score\"])\n",
    "\n",
    "    pd.DataFrame(\n",
    "        {\"score_name\": [full_score_name],\n",
    "        \"score_value\": [mean_score]}\n",
    "    ).to_csv(result_file, mode=\"a\", decimal=\",\", header=False, index=False)\n",
    "\n",
    "    return mean_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_params = {\n",
    "    \"svm\": (LinearSVC(), {\"linearsvc__C\": [0.001, 0.01, 0.1, 1, 10]}), \n",
    "    \"log\": (LogisticRegression(max_iter=800), {\"logisticregression__C\": [0.001, 0.01, 0.1, 1, 10]}), \n",
    "    \"mnb\": (MultinomialNB(), {\"multinomialnb__alpha\": [0.001, 0.01, 0.1, 1, 10]})\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file = \"./results/bow_EN.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_nltk_tokenizer(comment):\n",
    "    try:\n",
    "        return word_tokenize(comment)\n",
    "    except:\n",
    "        print(\"NLTK tokenization exception for\", comment)\n",
    "        try:\n",
    "            return word_tokenize(comment.strip(punctuation))\n",
    "        except:\n",
    "            print(\"NLTK tokenization withput punctuation exception for\", comment)\n",
    "            return comment.split(\" \")\n",
    "\n",
    "data[\"nltk_tokens\"] = data[\"Comment\"].apply(lambda comment: my_nltk_tokenizer(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "whitespace_tokenizer = WhitespaceTokenizer()\n",
    "\n",
    "\n",
    "def my_whitespace_tokenizer(comment):\n",
    "    return whitespace_tokenizer.tokenize(comment)\n",
    "\n",
    "data[\"whitespace_tokens\"] = data[\"Comment\"].apply(lambda comment: my_whitespace_tokenizer(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_word_tokenizer(comment):\n",
    "    token_pattern = re.compile(r\"(?u)\\b\\w\\w+\\b\")\n",
    "    return token_pattern.findall(comment)\n",
    "\n",
    "data[\"word_tokens\"] = data[\"Comment\"].apply(lambda comment: my_word_tokenizer(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_tokenize(tokens):\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, (estimator, hyper_params) in evaluation_params.items():\n",
    "    for y_name in [\"y8\", \"y6\", \"y2\"]:\n",
    "        # Try different tokenizers\n",
    "        scores = {}\n",
    "        for x_name in [\"nltk_tokens\", \"whitespace_tokens\",\"word_tokens\"]:\n",
    "            pipeline = make_pipeline(CountVectorizer(lowercase=False, tokenizer=dummy_tokenize),  estimator)\n",
    "            score = evaluate(data, x_name, y_name, result_file, x_name, model_name, pipeline, hyper_params)\n",
    "            scores[x_name] = score\n",
    "        print(f\"{model_name}-{y_name}-best-{max(scores, key=scores.get)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. snake_case/CamelCase/both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def snake_case_tokenize(tokens):\n",
    "    output_tokens = []\n",
    "    for token in tokens:\n",
    "        output_tokens.extend(token.split(\"_\"))\n",
    "\n",
    "    return list(filter(None, output_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"snake_nltk_tokens\"] = data[\"nltk_tokens\"].apply(lambda tokens: snake_case_tokenize(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def camel_case_tokenize(tokens):\n",
    "    try:\n",
    "        output_tokens = []\n",
    "        for token in tokens:\n",
    "            if len(token) == 0:\n",
    "                continue\n",
    "            new_tokens = []\n",
    "            new_tokens.append(str(token[0]))\n",
    "            for c in token[1:]:\n",
    "                if new_tokens[-1][-1].islower() and c.isupper():\n",
    "                    new_tokens.append(str(c))\n",
    "                else:\n",
    "                    new_tokens[-1] += c\n",
    "\n",
    "            output_tokens.extend(new_tokens)\n",
    "\n",
    "        return list(filter(None, output_tokens))\n",
    "    except:\n",
    "        print(\"-------------- CAMEL CASE ERROR ------------\")\n",
    "        print(tokens)\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"camel_nltk_tokens\"] = data[\"nltk_tokens\"].apply(lambda tokens: camel_case_tokenize(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"snake_camel_nltk_tokens\"] = data[\"snake_nltk_tokens\"].apply(lambda tokens: camel_case_tokenize(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, (estimator, hyper_params) in evaluation_params.items():\n",
    "    for y_name in [\"y8\", \"y6\", \"y2\"]:\n",
    "        # Try different cases\n",
    "        scores = {}\n",
    "        for x_name in [\"nltk_tokens\", \"snake_nltk_tokens\", \"camel_nltk_tokens\",\"snake_camel_nltk_tokens\"]:\n",
    "            pipeline = make_pipeline(CountVectorizer(lowercase=False, tokenizer=dummy_tokenize),  estimator)\n",
    "            score = evaluate(data, x_name, y_name, result_file, x_name, model_name, pipeline, hyper_params)\n",
    "            scores[x_name] = score\n",
    "        print(f\"{model_name}-{y_name}-best-{max(scores, key=scores.get)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Stemming/Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_en(tokens):\n",
    "    return [stemmer.stem(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"stem_nltk_tokens\"] = data[\"nltk_tokens\"].apply(lambda tokens: stem_en(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "text = \"studies studying cries cry\"\n",
    "\n",
    "def lemma_en(tokens):\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"lema_nltk_tokens\"] = data[\"nltk_tokens\"].apply(lambda tokens: lemma_en(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, (estimator, hyper_params) in evaluation_params.items():\n",
    "    for y_name in [\"y8\", \"y6\", \"y2\"]:\n",
    "        # Try different cases\n",
    "        scores = {}\n",
    "        for x_name in [\"nltk_tokens\", \"lema_nltk_tokens\", \"stem_nltk_tokens\"]:\n",
    "            pipeline = make_pipeline(CountVectorizer(lowercase=False, tokenizer=dummy_tokenize),  estimator)\n",
    "            score = evaluate(data, x_name, y_name, result_file, x_name, model_name, pipeline, hyper_params)\n",
    "            scores[x_name] = score\n",
    "        print(f\"{model_name}-{y_name}-best-{max(scores, key=scores.get)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"nltk_tokens_lower\"] = data[\"nltk_tokens\"].apply(lambda tokens: [token.lower() for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, (estimator, hyper_params) in evaluation_params.items():\n",
    "    for y_name in [\"y8\", \"y6\", \"y2\"]:\n",
    "        # Try different cases\n",
    "        scores = {}\n",
    "        for x_name in [\"nltk_tokens\", \"nltk_tokens_lower\"]:\n",
    "            pipeline = make_pipeline(CountVectorizer(lowercase=False, tokenizer=dummy_tokenize),  estimator)\n",
    "            score = evaluate(data, x_name, y_name, result_file, x_name, model_name, pipeline, hyper_params)\n",
    "            scores[x_name] = score\n",
    "        print(f\"{model_name}-{y_name}-best-{max(scores, key=scores.get)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Remove punctuation/numbers/both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"nltk_nopunctuation_tokens\"] = data[\"Comment\"].apply(lambda comment: my_nltk_tokenizer(re.sub(r\"[^\\w\\s]\", \" \", comment)))\n",
    "data[\"nltk_nonumbers_tokens\"] = data[\"Comment\"].apply(lambda comment: my_nltk_tokenizer(re.sub(r\"[0-9]+\", \" \", comment)))\n",
    "data[\"nltk_nopunctuationnumbers_tokens\"] = data[\"Comment\"].apply(lambda comment: my_nltk_tokenizer(re.sub(r\"[0-9]+\", \" \", re.sub(r\"[^\\w\\s]\", \" \", comment))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, (estimator, hyper_params) in evaluation_params.items():\n",
    "    for y_name in [\"y8\", \"y6\", \"y2\"]:\n",
    "        # Try different cases\n",
    "        scores = {}\n",
    "        for x_name in [\"nltk_nopunctuation_tokens\", \"nltk_nonumbers_tokens\", \"nltk_nopunctuationnumbers_tokens\"]:\n",
    "            pipeline = make_pipeline(CountVectorizer(lowercase=False, tokenizer=dummy_tokenize),  estimator)\n",
    "            score = evaluate(data, x_name, y_name, result_file, x_name, model_name, pipeline, hyper_params)\n",
    "            scores[x_name] = score\n",
    "        print(f\"{model_name}-{y_name}-best-{max(scores, key=scores.get)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Unigrams/Bigrams/Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, (estimator, hyper_params) in evaluation_params.items():\n",
    "    for y_name in [\"y8\", \"y6\", \"y2\"]:\n",
    "        # Try different cases\n",
    "        scores = {}\n",
    "        for x_name in [\"nltk_tokens\"]:\n",
    "            pipeline = make_pipeline(CountVectorizer(lowercase=False, tokenizer=dummy_tokenize, ngram_range=(1, 2)),  estimator)\n",
    "            score = evaluate(data, x_name, y_name, result_file, \"(1_2)\"+x_name, model_name, pipeline, hyper_params)\n",
    "            pipeline = make_pipeline(CountVectorizer(lowercase=False, tokenizer=dummy_tokenize, ngram_range=(1, 3)),  estimator)\n",
    "            score = evaluate(data, x_name, y_name, result_file, \"(1_3)\"+x_name, model_name, pipeline, hyper_params)\n",
    "            scores[x_name] = score\n",
    "        print(f\"{model_name}-{y_name}-best-{max(scores, key=scores.get)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. TfIdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, (estimator, hyper_params) in evaluation_params.items():\n",
    "    for y_name in [\"y8\", \"y6\", \"y2\"]:\n",
    "        # Try different cases\n",
    "        scores = {}\n",
    "        for x_name in [\"nltk_tokens\"]:\n",
    "            pipeline = make_pipeline(CountVectorizer(lowercase=False, tokenizer=dummy_tokenize), TfidfTransformer(use_idf=False), estimator)\n",
    "            score = evaluate(data, x_name, y_name, result_file, \"tf\"+x_name, model_name, pipeline, hyper_params)\n",
    "            pipeline = make_pipeline(CountVectorizer(lowercase=False, tokenizer=dummy_tokenize), TfidfTransformer(use_idf=True), estimator)\n",
    "            score = evaluate(data, x_name, y_name, result_file, \"tfidf\"+x_name, model_name, pipeline, hyper_params)\n",
    "            scores[x_name] = score\n",
    "        print(f\"{model_name}-{y_name}-best-{max(scores, key=scores.get)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Tfidf with bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, (estimator, hyper_params) in evaluation_params.items():\n",
    "    for y_name in [\"y8\", \"y6\", \"y2\"]:\n",
    "        # Try different cases\n",
    "        scores = {}\n",
    "        for x_name in [\"nltk_tokens\"]:\n",
    "            pipeline = make_pipeline(CountVectorizer(lowercase=False, tokenizer=dummy_tokenize, ngram_range=(1, 2)), TfidfTransformer(use_idf=False), estimator)\n",
    "            score = evaluate(data, x_name, y_name, result_file, \"(1_2)tf\"+x_name, model_name, pipeline, hyper_params)\n",
    "            pipeline = make_pipeline(CountVectorizer(lowercase=False, tokenizer=dummy_tokenize, ngram_range=(1, 2)), TfidfTransformer(use_idf=True), estimator)\n",
    "            score = evaluate(data, x_name, y_name, result_file, \"(1_2)tfidf\"+x_name, model_name, pipeline, hyper_params)\n",
    "            scores[x_name] = score\n",
    "        print(f\"{model_name}-{y_name}-best-{max(scores, key=scores.get)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per programming language analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best: NLTK tokens, bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file = \"bow_EN_per_language.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation  C-(1_2)nltk_tokens-svm-8\n",
      "Evaluation  C-(1_2)nltk_tokens-svm-6\n",
      "Evaluation  C-(1_2)nltk_tokens-svm-2\n",
      "Evaluation  C-(1_2)nltk_tokens-log-8\n",
      "Evaluation  C-(1_2)nltk_tokens-log-6\n",
      "Evaluation  C-(1_2)nltk_tokens-log-2\n",
      "Evaluation  C-(1_2)nltk_tokens-mnb-8\n",
      "Evaluation  C-(1_2)nltk_tokens-mnb-6\n",
      "Evaluation  C-(1_2)nltk_tokens-mnb-2\n",
      "Evaluation  C++-(1_2)nltk_tokens-svm-8\n",
      "Evaluation  C++-(1_2)nltk_tokens-svm-6\n",
      "Evaluation  C++-(1_2)nltk_tokens-svm-2\n",
      "Evaluation  C++-(1_2)nltk_tokens-log-8\n",
      "Evaluation  C++-(1_2)nltk_tokens-log-6\n",
      "Evaluation  C++-(1_2)nltk_tokens-log-2\n",
      "Evaluation  C++-(1_2)nltk_tokens-mnb-8\n",
      "Evaluation  C++-(1_2)nltk_tokens-mnb-6\n",
      "Evaluation  C++-(1_2)nltk_tokens-mnb-2\n",
      "Evaluation  C#-(1_2)nltk_tokens-svm-8\n",
      "Evaluation  C#-(1_2)nltk_tokens-svm-6\n",
      "Evaluation  C#-(1_2)nltk_tokens-svm-2\n",
      "Evaluation  C#-(1_2)nltk_tokens-log-8\n",
      "Evaluation  C#-(1_2)nltk_tokens-log-6\n",
      "Evaluation  C#-(1_2)nltk_tokens-log-2\n",
      "Evaluation  C#-(1_2)nltk_tokens-mnb-8\n",
      "Evaluation  C#-(1_2)nltk_tokens-mnb-6\n",
      "Evaluation  C#-(1_2)nltk_tokens-mnb-2\n",
      "Evaluation  Java-(1_2)nltk_tokens-svm-8\n",
      "Evaluation  Java-(1_2)nltk_tokens-svm-6\n",
      "Evaluation  Java-(1_2)nltk_tokens-svm-2\n",
      "Evaluation  Java-(1_2)nltk_tokens-log-8\n",
      "Evaluation  Java-(1_2)nltk_tokens-log-6\n",
      "Evaluation  Java-(1_2)nltk_tokens-log-2\n",
      "Evaluation  Java-(1_2)nltk_tokens-mnb-8\n",
      "Evaluation  Java-(1_2)nltk_tokens-mnb-6\n",
      "Evaluation  Java-(1_2)nltk_tokens-mnb-2\n",
      "Evaluation  JavaScript-(1_2)nltk_tokens-svm-8\n",
      "Evaluation  JavaScript-(1_2)nltk_tokens-svm-6\n",
      "Evaluation  JavaScript-(1_2)nltk_tokens-svm-2\n",
      "Evaluation  JavaScript-(1_2)nltk_tokens-log-8\n",
      "Evaluation  JavaScript-(1_2)nltk_tokens-log-6\n",
      "Evaluation  JavaScript-(1_2)nltk_tokens-log-2\n",
      "Evaluation  JavaScript-(1_2)nltk_tokens-mnb-8\n",
      "Evaluation  JavaScript-(1_2)nltk_tokens-mnb-6\n",
      "Evaluation  JavaScript-(1_2)nltk_tokens-mnb-2\n",
      "Evaluation  TypeScript-(1_2)nltk_tokens-svm-8\n",
      "Evaluation  TypeScript-(1_2)nltk_tokens-svm-6\n",
      "Evaluation  TypeScript-(1_2)nltk_tokens-svm-2\n",
      "Evaluation  TypeScript-(1_2)nltk_tokens-log-8\n",
      "Evaluation  TypeScript-(1_2)nltk_tokens-log-6\n",
      "Evaluation  TypeScript-(1_2)nltk_tokens-log-2\n",
      "Evaluation  TypeScript-(1_2)nltk_tokens-mnb-8\n",
      "Evaluation  TypeScript-(1_2)nltk_tokens-mnb-6\n",
      "Evaluation  TypeScript-(1_2)nltk_tokens-mnb-2\n",
      "Evaluation  PHP-(1_2)nltk_tokens-svm-8\n",
      "Evaluation  PHP-(1_2)nltk_tokens-svm-6\n",
      "Evaluation  PHP-(1_2)nltk_tokens-svm-2\n",
      "Evaluation  PHP-(1_2)nltk_tokens-log-8\n",
      "Evaluation  PHP-(1_2)nltk_tokens-log-6\n",
      "Evaluation  PHP-(1_2)nltk_tokens-log-2\n",
      "Evaluation  PHP-(1_2)nltk_tokens-mnb-8\n",
      "Evaluation  PHP-(1_2)nltk_tokens-mnb-6\n",
      "Evaluation  PHP-(1_2)nltk_tokens-mnb-2\n",
      "Evaluation  Python-(1_2)nltk_tokens-svm-8\n",
      "Evaluation  Python-(1_2)nltk_tokens-svm-6\n",
      "Evaluation  Python-(1_2)nltk_tokens-svm-2\n",
      "Evaluation  Python-(1_2)nltk_tokens-log-8\n",
      "Evaluation  Python-(1_2)nltk_tokens-log-6\n",
      "Evaluation  Python-(1_2)nltk_tokens-log-2\n",
      "Evaluation  Python-(1_2)nltk_tokens-mnb-8\n",
      "Evaluation  Python-(1_2)nltk_tokens-mnb-6\n",
      "Evaluation  Python-(1_2)nltk_tokens-mnb-2\n",
      "Evaluation  SQL-(1_2)nltk_tokens-svm-8\n",
      "Evaluation  SQL-(1_2)nltk_tokens-svm-6\n",
      "Evaluation  SQL-(1_2)nltk_tokens-svm-2\n",
      "Evaluation  SQL-(1_2)nltk_tokens-log-8\n",
      "Evaluation  SQL-(1_2)nltk_tokens-log-6\n",
      "Evaluation  SQL-(1_2)nltk_tokens-log-2\n",
      "Evaluation  SQL-(1_2)nltk_tokens-mnb-8\n",
      "Evaluation  SQL-(1_2)nltk_tokens-mnb-6\n",
      "Evaluation  SQL-(1_2)nltk_tokens-mnb-2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "for lang_name in ['C', 'C++', 'C#', 'Java', 'JavaScript', 'TypeScript', 'PHP', 'Python', 'SQL']:\n",
    "    data_train = data[data.ProgrammingLanguageID != lang_name]\n",
    "    data_test = data[data.ProgrammingLanguageID == lang_name]\n",
    "\n",
    "    for model_name, (estimator, _) in evaluation_params.items():\n",
    "        for y_name in [\"y8\", \"y6\", \"y2\"]:\n",
    "            # Try different cases\n",
    "            for x_name in [\"nltk_tokens\"]:\n",
    "                X = data_train[x_name]\n",
    "                y = data_train[y_name]\n",
    "                score_name = make_score_name(lang_name+\"-(1_2)\"+x_name, model_name, y.nunique())\n",
    "                print(\"Evaluation \", score_name)\n",
    "\n",
    "                pipeline = make_pipeline(CountVectorizer(lowercase=False, tokenizer=dummy_tokenize, ngram_range=(1, 2)),  estimator)\n",
    "                pipeline.fit(X, y)\n",
    "\n",
    "                X_test = data_test[x_name]\n",
    "                y_test = data_test[y_name]\n",
    "                y_pred = pipeline.predict(data_test[x_name])\n",
    "                score = f1_score(y_test, y_pred, average=\"macro\")\n",
    "\n",
    "                pd.DataFrame(\n",
    "                    {\"score_name\": [score_name],\n",
    "                    \"score_value\": [score]}\n",
    "                ).to_csv(result_file, mode=\"a\", decimal=\",\", header=False, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
