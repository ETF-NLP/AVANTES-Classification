{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "from nltk.tokenize import word_tokenize, WhitespaceTokenizer\n",
    "import re\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_validate\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from tqdm import tqdm\n",
    "import classla\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"./data/corpus.csv\"\n",
    "corpus = pd.read_csv(\"./data/corpus.csv\", dtype=\"string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SR data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = \"SR\"\n",
    "data = corpus[corpus.NaturalLanguageID == language]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove ide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data.y8 != \"ide\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_tokens(name, data, lang=language):\n",
    "    output_file_name = f\"data/tokens/{lang}_{name}.csv\"\n",
    "    \n",
    "    # X_temp = X.apply(lambda tokens: \" \".join(tokens))\n",
    "    \n",
    "    print(f\"Writing {name} {data.shape} to {output_file_name}.\")\n",
    "    data.apply(lambda tokens: \" \".join(tokens)).to_csv(output_file_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_results(result_file, score_name, score_value):\n",
    "    pd.DataFrame(\n",
    "        {\"score_name\": [score_name],\n",
    "        \"score_value\": [score_value]}\n",
    "    ).to_csv(result_file, mode=\"a\", decimal=\",\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_score_name(score_name, model_name, num_classes):\n",
    "    return f\"{score_name}-{model_name}-{num_classes}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data, x_column_name, y_column_name, result_file, score_name, model_name, estimator, hyper_params):\n",
    "    print(estimator)\n",
    "    \n",
    "    X = data[x_column_name]\n",
    "    y = data[y_column_name]\n",
    "\n",
    "    full_score_name = make_score_name(score_name, model_name, y.nunique())\n",
    "    print(f\"--------Evaluating {full_score_name} --------\")\n",
    "    gs_estimator = GridSearchCV(\n",
    "        estimator, hyper_params, scoring=\"f1_macro\", cv=StratifiedKFold(n_splits=10, shuffle=True, random_state=42), verbose=0, n_jobs=-1)\n",
    "\n",
    "    scores = cross_validate(\n",
    "        gs_estimator, X, y, scoring=\"f1_macro\", cv=StratifiedKFold(n_splits=10, shuffle=True, random_state=42), verbose=0, n_jobs=-1)\n",
    "    mean_score = np.mean(scores[\"test_score\"])\n",
    "\n",
    "    pd.DataFrame(\n",
    "        {\"score_name\": [full_score_name],\n",
    "        \"score_value\": [mean_score]}\n",
    "    ).to_csv(result_file, mode=\"a\", decimal=\",\", header=False, index=False)\n",
    "\n",
    "    return mean_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_params = {\n",
    "    \"svm\": (LinearSVC(), {\"linearsvc__C\": [0.001, 0.01, 0.1, 1, 10]}), \n",
    "    \"log\": (LogisticRegression(max_iter=800), {\"logisticregression__C\": [0.001, 0.01, 0.1, 1, 10]}), \n",
    "    \"mnb\": (MultinomialNB(), {\"multinomialnb__alpha\": [0.001, 0.01, 0.1, 1, 10]})\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file = \"./results/bow_SR.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "classla.download('sr', type='nonstandard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr_pipeline = classla.Pipeline(\"sr\", type=\"nonstandard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classla_tokenize(comment):\n",
    "    try:\n",
    "        doc = sr_pipeline(comment)\n",
    "        return list([word.text for word in doc.iter_words()])\n",
    "    except:\n",
    "        print(f\"SR tokenize ERROR for comment: {comment}\")\n",
    "        return comment.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "data[\"classla_tokens\"] = data[\"Comment\"].progress_apply(lambda comment: classla_tokenize(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_tokens(\"classla\", data[\"classla_tokens\"], \"SR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whitespace_tokenizer = WhitespaceTokenizer()\n",
    "\n",
    "\n",
    "def my_whitespace_tokenizer(comment):\n",
    "    return whitespace_tokenizer.tokenize(comment)\n",
    "\n",
    "data[\"whitespace_tokens\"] = data[\"Comment\"].apply(lambda comment: my_whitespace_tokenizer(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_word_tokenizer(comment):\n",
    "    token_pattern = re.compile(r\"(?u)\\b\\w\\w+\\b\")\n",
    "    return token_pattern.findall(comment)\n",
    "\n",
    "data[\"word_tokens\"] = data[\"Comment\"].apply(lambda comment: my_word_tokenizer(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_tokenize(tokens):\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, (estimator, hyper_params) in evaluation_params.items():\n",
    "    for y_name in [\"y8\", \"y6\", \"y2\"]:\n",
    "        # Try different tokenizers\n",
    "        scores = {}\n",
    "        for x_name in [\"classla_tokens\", \"whitespace_tokens\",\"word_tokens\"]:\n",
    "            pipeline = make_pipeline(CountVectorizer(lowercase=False, tokenizer=dummy_tokenize),  estimator)\n",
    "            score = evaluate(data, x_name, y_name, result_file, x_name, model_name, pipeline, hyper_params)\n",
    "            scores[x_name] = score\n",
    "        print(f\"{model_name}-{y_name}-best-{max(scores, key=scores.get)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. snake_case/CamelCase/both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def snake_case_tokenize(tokens):\n",
    "    output_tokens = []\n",
    "    for token in tokens:\n",
    "        output_tokens.extend(token.split(\"_\"))\n",
    "\n",
    "    return list(filter(None, output_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"snake_classla_tokens\"] = data[\"classla_tokens\"].apply(lambda tokens: snake_case_tokenize(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def camel_case_tokenize(tokens):\n",
    "    try:\n",
    "        output_tokens = []\n",
    "        for token in tokens:\n",
    "            if len(token) == 0:\n",
    "                continue\n",
    "            new_tokens = []\n",
    "            new_tokens.append(str(token[0]))\n",
    "            for c in token[1:]:\n",
    "                if new_tokens[-1][-1].islower() and c.isupper():\n",
    "                    new_tokens.append(str(c))\n",
    "                else:\n",
    "                    new_tokens[-1] += c\n",
    "\n",
    "            output_tokens.extend(new_tokens)\n",
    "\n",
    "        return list(filter(None, output_tokens))\n",
    "    except:\n",
    "        print(\"-------------- CAMEL CASE ERROR ------------\")\n",
    "        print(tokens)\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"camel_classla_tokens\"] = data[\"classla_tokens\"].apply(lambda tokens: camel_case_tokenize(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"snake_camel_classla_tokens\"] = data[\"snake_classla_tokens\"].apply(lambda tokens: camel_case_tokenize(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, (estimator, hyper_params) in evaluation_params.items():\n",
    "    for y_name in [\"y8\", \"y6\", \"y2\"]:\n",
    "        # Try different cases\n",
    "        scores = {}\n",
    "        for x_name in [\"snake_classla_tokens\", \"camel_classla_tokens\",\"snake_camel_classla_tokens\"]:\n",
    "            pipeline = make_pipeline(CountVectorizer(lowercase=False, tokenizer=dummy_tokenize),  estimator)\n",
    "            score = evaluate(data, x_name, y_name, result_file, x_name, model_name, pipeline, hyper_params)\n",
    "            scores[x_name] = score\n",
    "        print(f\"{model_name}-{y_name}-best-{max(scores, key=scores.get)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Stemming/Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_df = pd.read_csv(\"data/tokens/SR_classla_stemmed.csv\", header=0, dtype=\"string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_df.index = data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.join(stemmed_df).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"stemmed_classla_tokens\"] = stemmed_df[\"stemmed_classla_tokens\"].apply(lambda comment: comment.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr_pretokenized_pipeline = classla.Pipeline(\n",
    "    \"sr\", type=\"nonstandard\", tokenize_pretokenized=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma_sr(tokens):\n",
    "    try:\n",
    "        doc = sr_pretokenized_pipeline(\" \".join(tokens))\n",
    "        return list([word.lemma for word in doc.iter_words()])\n",
    "    except:\n",
    "        print(f\"Lema SR error for tokens: {tokens}\")\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "data[\"lema_classla_tokens\"] = data[\"classla_tokens\"].progress_apply(lambda tokens: lemma_sr(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, (estimator, hyper_params) in evaluation_params.items():\n",
    "    for y_name in [\"y8\", \"y6\", \"y2\"]:\n",
    "        # Try different cases\n",
    "        scores = {}\n",
    "        for x_name in [\"lema_classla_tokens\", \"stemmed_classla_tokens\"]:\n",
    "            pipeline = make_pipeline(CountVectorizer(lowercase=False, tokenizer=dummy_tokenize),  estimator)\n",
    "            score = evaluate(data, x_name, y_name, result_file, x_name, model_name, pipeline, hyper_params)\n",
    "            scores[x_name] = score\n",
    "        print(f\"{model_name}-{y_name}-best-{max(scores, key=scores.get)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"classla_tokens_lower\"] = data[\"classla_tokens\"].apply(lambda tokens: [token.lower() for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, (estimator, hyper_params) in evaluation_params.items():\n",
    "    for y_name in [\"y8\", \"y6\", \"y2\"]:\n",
    "        # Try different cases\n",
    "        scores = {}\n",
    "        for x_name in [\"classla_tokens_lower\"]:\n",
    "            pipeline = make_pipeline(CountVectorizer(lowercase=False, tokenizer=dummy_tokenize),  estimator)\n",
    "            score = evaluate(data, x_name, y_name, result_file, x_name, model_name, pipeline, hyper_params)\n",
    "            scores[x_name] = score\n",
    "        print(f\"{model_name}-{y_name}-best-{max(scores, key=scores.get)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Remove punctuation/numbers/both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "data[\"classla_nopunctuation_tokens\"] = data[\"Comment\"].progress_apply(lambda comment: classla_tokenize(re.sub(r\"[^\\w\\s]\", \" \", comment)))\n",
    "data[\"classla_nonumbers_tokens\"] = data[\"Comment\"].progress_apply(lambda comment: classla_tokenize(re.sub(r\"[0-9]+\", \" \", comment)))\n",
    "data[\"classla_nopunctuationnumbers_tokens\"] = data[\"Comment\"].progress_apply(lambda comment: classla_tokenize(re.sub(r\"[0-9]+\", \" \", re.sub(r\"[^\\w\\s]\", \" \", comment))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, (estimator, hyper_params) in evaluation_params.items():\n",
    "    for y_name in [\"y8\", \"y6\", \"y2\"]:\n",
    "        # Try different cases\n",
    "        scores = {}\n",
    "        for x_name in [\"classla_nopunctuation_tokens\", \"classla_nonumbers_tokens\", \"classla_nopunctuationnumbers_tokens\"]:\n",
    "            pipeline = make_pipeline(CountVectorizer(lowercase=False, tokenizer=dummy_tokenize),  estimator)\n",
    "            score = evaluate(data, x_name, y_name, result_file, x_name, model_name, pipeline, hyper_params)\n",
    "            scores[x_name] = score\n",
    "        print(f\"{model_name}-{y_name}-best-{max(scores, key=scores.get)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Bigrams/Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, (estimator, hyper_params) in evaluation_params.items():\n",
    "    for y_name in [\"y8\", \"y6\", \"y2\"]:\n",
    "        # Try different cases\n",
    "        scores = {}\n",
    "        for x_name in [\"classla_tokens\"]:\n",
    "            pipeline = make_pipeline(CountVectorizer(lowercase=False, tokenizer=dummy_tokenize, ngram_range=(1, 2)),  estimator)\n",
    "            score = evaluate(data, x_name, y_name, result_file, \"(1_2)\"+x_name, model_name, pipeline, hyper_params)\n",
    "            pipeline = make_pipeline(CountVectorizer(lowercase=False, tokenizer=dummy_tokenize, ngram_range=(1, 3)),  estimator)\n",
    "            score = evaluate(data, x_name, y_name, result_file, \"(1_3)\"+x_name, model_name, pipeline, hyper_params)\n",
    "            scores[x_name] = score\n",
    "        print(f\"{model_name}-{y_name}-best-{max(scores, key=scores.get)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Tf/Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, (estimator, hyper_params) in evaluation_params.items():\n",
    "    for y_name in [\"y8\", \"y6\", \"y2\"]:\n",
    "        # Try different cases\n",
    "        scores = {}\n",
    "        for x_name in [\"classla_tokens\"]:\n",
    "            pipeline = make_pipeline(CountVectorizer(lowercase=False, tokenizer=dummy_tokenize), TfidfTransformer(use_idf=False), estimator)\n",
    "            score = evaluate(data, x_name, y_name, result_file, \"tf_\"+x_name, model_name, pipeline, hyper_params)\n",
    "            pipeline = make_pipeline(CountVectorizer(lowercase=False, tokenizer=dummy_tokenize), TfidfTransformer(use_idf=True), estimator)\n",
    "            score = evaluate(data, x_name, y_name, result_file, \"tfidf_\"+x_name, model_name, pipeline, hyper_params)\n",
    "            scores[x_name] = score\n",
    "        print(f\"{model_name}-{y_name}-best-{max(scores, key=scores.get)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per programming language analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best: ReLDi/classla tokens, TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file = \"./results/bow_SR_per_language.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "classla_df = pd.read_csv(\"data/tokens/SR_classla.csv\", header=0, dtype=\"string\")\n",
    "classla_df.index = data.index\n",
    "data.join(classla_df)\n",
    "data[\"classla_tokens\"] = classla_df[\"classla_tokens\"].apply(lambda comment: comment.split(\" \"))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "for lang_name in ['C', 'C++', 'C#', 'Java', 'TypeScript', 'Python', 'SQL']:\n",
    "    data_train = data[data.ProgrammingLanguageID != lang_name]\n",
    "    data_test = data[data.ProgrammingLanguageID == lang_name]\n",
    "\n",
    "    for model_name, (estimator, hyper_params) in evaluation_params.items():\n",
    "        for y_name in [\"y8\", \"y6\", \"y2\"]:\n",
    "            # Try different cases\n",
    "            for x_name in [\"classla_tokens\"]:\n",
    "                X = data_train[x_name]\n",
    "                y = data_train[y_name]\n",
    "                score_name = make_score_name(lang_name+\"-tf_\"+x_name, model_name, y.nunique())\n",
    "                print(\"Evaluation \", score_name)\n",
    "\n",
    "                pipeline = make_pipeline(CountVectorizer(lowercase=False, tokenizer=dummy_tokenize), TfidfTransformer(use_idf=False), estimator)\n",
    "                gs_estimator = GridSearchCV(pipeline, hyper_params, scoring=\"f1_macro\", cv=StratifiedKFold(n_splits=10, shuffle=True, random_state=42), verbose=0, n_jobs=-1)\n",
    "                gs_estimator.fit(X, y)\n",
    "\n",
    "                X_test = data_test[x_name]\n",
    "                y_test = data_test[y_name]\n",
    "                y_pred = gs_estimator.predict(X_test)\n",
    "                score = f1_score(y_test, y_pred, average=\"macro\")\n",
    "\n",
    "                pd.DataFrame(\n",
    "                    {\"score_name\": [score_name],\n",
    "                    \"score_value\": [score]}\n",
    "                ).to_csv(result_file, mode=\"a\", decimal=\",\", header=False, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top absolute 100 for svm and log and top 100 tokens for each class for mnb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_tokens = pd.DataFrame()\n",
    "\n",
    "models = {\"svm\": (LinearSVC(max_iter=2000), \"linearsvc\", {\"linearsvc__C\": [0.001, 0.01, 0.1, 1, 10]}),\n",
    "          \"log\": (LogisticRegression(max_iter=800), \"logisticregression\", {\"logisticregression__C\": [0.001, 0.01, 0.1, 1, 10]}),\n",
    "          \"mnb\": (MultinomialNB(), \"multinomialnb\", {\"multinomialnb__alpha\": [0.001, 0.01, 0.1, 1, 10]})}\n",
    "\n",
    "for model_name, (estimator, model_name_in_pipeline, hyper_params) in models.items():\n",
    "    for y_name in [\"y8\", \"y6\", \"y2\"]:\n",
    "        for x_name in [\"classla_tokens\"]:\n",
    "            X = data[x_name]\n",
    "            y = data[y_name]\n",
    "\n",
    "            pipeline = make_pipeline(CountVectorizer(lowercase=False, tokenizer=dummy_tokenize), TfidfTransformer(use_idf=False), estimator)\n",
    "            gs_estimator = GridSearchCV(\n",
    "                pipeline, hyper_params, scoring=\"f1_macro\", cv=StratifiedKFold(n_splits=10, shuffle=True, random_state=42), verbose=0, n_jobs=-1)\n",
    "            result = gs_estimator.fit(X, y)\n",
    "\n",
    "            col_name = f\"{model_name}_{y_name}\"\n",
    "            if model_name == \"mnb\":\n",
    "                est = gs_estimator.best_estimator_[model_name_in_pipeline]\n",
    "                for row, cls in zip(est.feature_log_prob_, est.classes_):\n",
    "                    weights = []\n",
    "                    tokens = []\n",
    "                    for i in (row).argsort()[::-1][:100]:\n",
    "                        weights.append(row[i])\n",
    "                        tokens.append(gs_estimator.best_estimator_['countvectorizer'].get_feature_names_out()[i])\n",
    "                    top_tokens[f\"{col_name}_{cls}_W\"] = weights\n",
    "                    top_tokens[f\"{col_name}_{cls}_T\"] = tokens\n",
    "            else:\n",
    "                weights = []\n",
    "                tokens = []\n",
    "                for i in np.absolute(gs_estimator.best_estimator_[model_name_in_pipeline].coef_[0]).argsort()[::-1][:100]:\n",
    "                    weights.append(gs_estimator.best_estimator_[model_name_in_pipeline].coef_[0][i])\n",
    "                    tokens.append(gs_estimator.best_estimator_['countvectorizer'].get_feature_names_out()[i])\n",
    "                top_tokens[f\"{col_name}_W\"] = weights\n",
    "                top_tokens[f\"{col_name}_T\"] = tokens\n",
    "\n",
    "            print(f\"Done {col_name}\")\n",
    "top_tokens.to_csv(\"./results/bow_SR_top_tokens.csv\", index=False, mode=\"a\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
