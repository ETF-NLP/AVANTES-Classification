{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_validate\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "from nltk.tokenize import word_tokenize, WhitespaceTokenizer\n",
    "import re\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_validate\n",
    "from sklearn.pipeline import make_pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"./data/corpus.csv\"\n",
    "corpus = pd.read_csv(\"./data/corpus.csv\", dtype=\"string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = \"EN\"\n",
    "data = corpus[corpus.NaturalLanguageID == language]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data[[\"Comment\", \"y8\", \"y6\", \"y2\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_tokens = 0\n",
    "total_tokens_without_embedding = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comment_vector(embeddings, comment):\n",
    "    global total_tokens, total_tokens_without_embedding\n",
    "\n",
    "    comment_vector = []\n",
    "    num_tokens = 0\n",
    "\n",
    "    for token in comment:\n",
    "        total_tokens += 1\n",
    "        try:\n",
    "            if num_tokens == 0:\n",
    "                comment_vector = embeddings[token]\n",
    "            else:\n",
    "                comment_vector = np.add(comment_vector, embeddings[token])\n",
    "            num_tokens += 1\n",
    "        except:\n",
    "            total_tokens_without_embedding += 1\n",
    "            pass\n",
    "    if num_tokens == 0:\n",
    "        return np.nan\n",
    "    return np.asarray(comment_vector) / num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(comments, embeddings):\n",
    "    global total_tokens, total_tokens_without_embedding\n",
    "\n",
    "    total_tokens = 0\n",
    "    total_tokens_without_embedding = 0\n",
    "\n",
    "    X_vectors = comments.apply(lambda comment: get_comment_vector(embeddings, comment))\n",
    "    X_vectors = X_vectors.apply(pd.Series)\n",
    "\n",
    "    nan_comments_mask = X_vectors.isnull().iloc[:, 0]\n",
    "    num_nan_comments = nan_comments_mask.sum()\n",
    "    print(f\"{num_nan_comments} comments have no embeddings\")\n",
    "\n",
    "    print (f\"Total tokens {total_tokens}, out of whicih {total_tokens_without_embedding} do not have embeddings -- {total_tokens_without_embedding/total_tokens*100:.4}%.\")\n",
    "    return X_vectors, nan_comments_mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_results(result_file, score_name, score_value):\n",
    "    pd.DataFrame(\n",
    "        {\"score_name\": [score_name],\n",
    "        \"score_value\": [score_value]}\n",
    "    ).to_csv(result_file, mode=\"a\", decimal=\",\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_score_name(score_name, model_name, num_classes):\n",
    "    return f\"{score_name}-{model_name}-{num_classes}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data, x_column_name, y_column_name, result_file, score_name, model_name, estimator, hyper_params, embeddings):\n",
    "    print(estimator)\n",
    "    \n",
    "    X = data[x_column_name]\n",
    "    y = data[y_column_name]\n",
    "\n",
    "    # Vectorize.\n",
    "    X, nan_comments_mask = vectorize(X, embeddings)\n",
    "    X = X[~nan_comments_mask]\n",
    "    y = y[~nan_comments_mask]\n",
    "\n",
    "    full_score_name = make_score_name(score_name, model_name, y.nunique())\n",
    "    print(f\"--------Evaluating {full_score_name} --------\")\n",
    "    gs_estimator = GridSearchCV(\n",
    "        estimator, hyper_params, scoring=\"f1_macro\", cv=StratifiedKFold(n_splits=10, shuffle=True, random_state=42), verbose=0, n_jobs=-1)\n",
    "\n",
    "    scores = cross_validate(\n",
    "        gs_estimator, X, y, scoring=\"f1_macro\", cv=StratifiedKFold(n_splits=10, shuffle=True, random_state=42), verbose=0, n_jobs=-1)\n",
    "    mean_score = np.mean(scores[\"test_score\"])\n",
    "\n",
    "    pd.DataFrame(\n",
    "        {\"score_name\": [full_score_name],\n",
    "        \"score_value\": [mean_score]}\n",
    "    ).to_csv(result_file, mode=\"a\", decimal=\",\", header=False, index=False)\n",
    "\n",
    "    return mean_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_params = {\n",
    "    \"svm\": (LinearSVC(), {\"C\": [0.001, 0.01, 0.1, 1, 10]}), \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file = \"./results/boe_EN.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read embeddings wiki_news_300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_file, vectors_name = (\"embeddings/wiki-news-300d-1M.vec\", \"wiki_news_300\")\n",
    "print(f\"Start reading {embedding_file}.\")\n",
    "embeddings_wiki = KeyedVectors.load_word2vec_format(embedding_file)\n",
    "print(f\"End reading {embedding_file}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_nltk_tokenizer(comment):\n",
    "    try:\n",
    "        return word_tokenize(comment)\n",
    "    except:\n",
    "        print(\"NLTK tokenization exception for\", comment)\n",
    "        try:\n",
    "            return word_tokenize(comment.strip(punctuation))\n",
    "        except:\n",
    "            print(\"NLTK tokenization withput punctuation exception for\", comment)\n",
    "            return comment.split(\" \")\n",
    "\n",
    "data[\"nltk_tokens\"] = data[\"Comment\"].apply(lambda comment: my_nltk_tokenizer(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "whitespace_tokenizer = WhitespaceTokenizer()\n",
    "\n",
    "\n",
    "def my_whitespace_tokenizer(comment):\n",
    "    return whitespace_tokenizer.tokenize(comment)\n",
    "\n",
    "data[\"whitespace_tokens\"] = data[\"Comment\"].apply(lambda comment: my_whitespace_tokenizer(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_word_tokenizer(comment):\n",
    "    token_pattern = re.compile(r\"(?u)\\b\\w\\w+\\b\")\n",
    "    return token_pattern.findall(comment)\n",
    "\n",
    "data[\"word_tokens\"] = data[\"Comment\"].apply(lambda comment: my_word_tokenizer(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, (estimator, hyper_params) in evaluation_params.items():\n",
    "    for y_name in [\"y8\", \"y6\", \"y2\"]:\n",
    "        # Try different tokenizers\n",
    "        scores = {}\n",
    "        for x_name in [\"nltk_tokens\", \"whitespace_tokens\",\"word_tokens\"]:\n",
    "            score = evaluate(data, x_name, y_name, result_file, vectors_name+\"-\"+x_name, model_name, estimator, hyper_params, embeddings_wiki)\n",
    "            scores[x_name] = score\n",
    "        print(f\"{model_name}-{y_name}-best-{max(scores, key=scores.get)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. snake_case/CamelCase/both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def snake_case_tokenize(tokens):\n",
    "    output_tokens = []\n",
    "    for token in tokens:\n",
    "        output_tokens.extend(token.split(\"_\"))\n",
    "\n",
    "    return list(filter(None, output_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"snake_nltk_tokens\"] = data[\"nltk_tokens\"].apply(lambda tokens: snake_case_tokenize(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def camel_case_tokenize(tokens):\n",
    "    try:\n",
    "        output_tokens = []\n",
    "        for token in tokens:\n",
    "            if len(token) == 0:\n",
    "                continue\n",
    "            new_tokens = []\n",
    "            new_tokens.append(str(token[0]))\n",
    "            for c in token[1:]:\n",
    "                if new_tokens[-1][-1].islower() and c.isupper():\n",
    "                    new_tokens.append(str(c))\n",
    "                else:\n",
    "                    new_tokens[-1] += c\n",
    "\n",
    "            output_tokens.extend(new_tokens)\n",
    "\n",
    "        return list(filter(None, output_tokens))\n",
    "    except:\n",
    "        print(\"-------------- CAMEL CASE ERROR ------------\")\n",
    "        print(tokens)\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"camel_nltk_tokens\"] = data[\"nltk_tokens\"].apply(lambda tokens: camel_case_tokenize(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"snake_camel_nltk_tokens\"] = data[\"snake_nltk_tokens\"].apply(lambda tokens: camel_case_tokenize(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, (estimator, hyper_params) in evaluation_params.items():\n",
    "    for y_name in [\"y8\", \"y6\", \"y2\"]:\n",
    "        scores = {}\n",
    "        for x_name in [\"snake_nltk_tokens\", \"camel_nltk_tokens\",\"snake_camel_nltk_tokens\"]:\n",
    "            score = evaluate(data, x_name, y_name, result_file, vectors_name+\"-\"+x_name, model_name, estimator, hyper_params, embeddings_wiki)\n",
    "            scores[x_name] = score\n",
    "        print(f\"{model_name}-{y_name}-best-{max(scores, key=scores.get)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Stemming/Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_en(tokens):\n",
    "    return [stemmer.stem(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"stem_nltk_tokens\"] = data[\"nltk_tokens\"].apply(lambda tokens: stem_en(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "text = \"studies studying cries cry\"\n",
    "\n",
    "def lemma_en(tokens):\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"lema_nltk_tokens\"] = data[\"nltk_tokens\"].apply(lambda tokens: lemma_en(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, (estimator, hyper_params) in evaluation_params.items():\n",
    "    for y_name in [\"y8\", \"y6\", \"y2\"]:\n",
    "        scores = {}\n",
    "        for x_name in [\"lema_nltk_tokens\", \"stem_nltk_tokens\"]:\n",
    "            score = evaluate(data, x_name, y_name, result_file, vectors_name+\"-\"+x_name, model_name, estimator, hyper_params, embeddings_wiki)\n",
    "            scores[x_name] = score\n",
    "        print(f\"{model_name}-{y_name}-best-{max(scores, key=scores.get)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"lema_nltk_lower_tokens\"] = data[\"lema_nltk_tokens\"].apply(lambda tokens: [token.lower() for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, (estimator, hyper_params) in evaluation_params.items():\n",
    "    for y_name in [\"y8\", \"y6\", \"y2\"]:\n",
    "        scores = {}\n",
    "        for x_name in [\"lema_nltk_lower_tokens\"]:\n",
    "            score = evaluate(data, x_name, y_name, result_file, vectors_name+\"-\"+x_name, model_name, estimator, hyper_params, embeddings_wiki)\n",
    "            scores[x_name] = score\n",
    "        print(f\"{model_name}-{y_name}-best-{max(scores, key=scores.get)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Without lema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"nltk_lower_tokens\"] = data[\"nltk_tokens\"].apply(lambda tokens: [token.lower() for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, (estimator, hyper_params) in evaluation_params.items():\n",
    "    for y_name in [\"y8\", \"y6\", \"y2\"]:\n",
    "        scores = {}\n",
    "        for x_name in [\"nltk_lower_tokens\"]:\n",
    "            score = evaluate(data, x_name, y_name, result_file, vectors_name+\"-\"+x_name, model_name, estimator, hyper_params, embeddings_wiki)\n",
    "            scores[x_name] = score\n",
    "        print(f\"{model_name}-{y_name}-best-{max(scores, key=scores.get)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Remove punctuation/numbers/both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"lema_nltk_nopunctuation_tokens\"] = data[\"Comment\"].apply(lambda comment: lemma_en(my_nltk_tokenizer(re.sub(r\"[^\\w\\s]\", \" \", comment))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"lema_nltk_nonumbers_tokens\"] = data[\"Comment\"].apply(lambda comment: lemma_en(my_nltk_tokenizer(re.sub(r\"[0-9]+\", \" \", comment))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"lema_nltk_nopunctuationnumbers_tokens\"] = data[\"Comment\"].apply(lambda comment: lemma_en(my_nltk_tokenizer(re.sub(r\"[0-9]+\", \" \", re.sub(r\"[^\\w\\s]\", \" \", comment)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, (estimator, hyper_params) in evaluation_params.items():\n",
    "    for y_name in [\"y8\", \"y6\", \"y2\"]:\n",
    "        scores = {}\n",
    "        for x_name in [\"lema_nltk_nopunctuation_tokens\", \"lema_nltk_nonumbers_tokens\", \"lema_nltk_nopunctuationnumbers_tokens\"]:\n",
    "            score = evaluate(data, x_name, y_name, result_file, vectors_name+\"-\"+x_name, model_name, estimator, hyper_params, embeddings_wiki)\n",
    "            scores[x_name] = score\n",
    "        print(f\"{model_name}-{y_name}-best-{max(scores, key=scores.get)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_wiki[\"#\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Without lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, (estimator, hyper_params) in evaluation_params.items():\n",
    "    for y_name in [\"y8\", \"y6\", \"y2\"]:\n",
    "        scores = {}\n",
    "        for x_name in [\"nltk_nopunctuation_tokens\", \"nltk_nonumbers_tokens\", \"nltk_nopunctuationnumbers_tokens\"]:\n",
    "            score = evaluate(data, x_name, y_name, result_file, vectors_name+\"-\"+x_name, model_name, estimator, hyper_params, embeddings_wiki)\n",
    "            scores[x_name] = score\n",
    "        print(f\"{model_name}-{y_name}-best-{max(scores, key=scores.get)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read embeddings crawl_300d_2M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_file, vectors_name = (\"embeddings/crawl-300d-2M.vec\", \"crawl_300d_2M\")\n",
    "print(f\"Start reading {embedding_file}.\")\n",
    "embeddings_crawl = KeyedVectors.load_word2vec_format(embedding_file)\n",
    "print(f\"End reading {embedding_file}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, (estimator, hyper_params) in evaluation_params.items():\n",
    "    for y_name in [\"y8\", \"y6\", \"y2\"]:\n",
    "        # Try different tokenizers\n",
    "        scores = {}\n",
    "        for x_name in [\"nltk_tokens\", \"whitespace_tokens\",\"word_tokens\"]:\n",
    "            score = evaluate(data, x_name, y_name, result_file, vectors_name+\"-\"+x_name, model_name, estimator, hyper_params, embeddings_crawl)\n",
    "            scores[x_name] = score\n",
    "        print(f\"{model_name}-{y_name}-best-{max(scores, key=scores.get)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. snake_case/CamelCase/both\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, (estimator, hyper_params) in evaluation_params.items():\n",
    "    for y_name in [\"y8\", \"y6\", \"y2\"]:\n",
    "        scores = {}\n",
    "        for x_name in [\"snake_nltk_tokens\", \"camel_nltk_tokens\",\"snake_camel_nltk_tokens\"]:\n",
    "            score = evaluate(data, x_name, y_name, result_file, vectors_name+\"-\"+x_name, model_name, estimator, hyper_params, embeddings_crawl)\n",
    "            scores[x_name] = score\n",
    "        print(f\"{model_name}-{y_name}-best-{max(scores, key=scores.get)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Stemming/Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, (estimator, hyper_params) in evaluation_params.items():\n",
    "    for y_name in [\"y8\", \"y6\", \"y2\"]:\n",
    "        scores = {}\n",
    "        for x_name in [\"lema_nltk_tokens\", \"stem_nltk_tokens\"]:\n",
    "            score = evaluate(data, x_name, y_name, result_file, vectors_name+\"-\"+x_name, model_name, estimator, hyper_params, embeddings_crawl)\n",
    "            scores[x_name] = score\n",
    "        print(f\"{model_name}-{y_name}-best-{max(scores, key=scores.get)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"nltk_lower_tokens\"] = data[\"nltk_tokens\"].apply(lambda tokens: [token.lower() for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, (estimator, hyper_params) in evaluation_params.items():\n",
    "    for y_name in [\"y8\", \"y6\", \"y2\"]:\n",
    "        scores = {}\n",
    "        for x_name in [\"nltk_lower_tokens\"]:\n",
    "            score = evaluate(data, x_name, y_name, result_file, vectors_name+\"-\"+x_name, model_name, estimator, hyper_params, embeddings_crawl)\n",
    "            scores[x_name] = score\n",
    "        print(f\"{model_name}-{y_name}-best-{max(scores, key=scores.get)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Remove punctuation/numbers/both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"nltk_nopunctuation_tokens\"] = data[\"Comment\"].apply(lambda comment: my_nltk_tokenizer(re.sub(r\"[^\\w\\s]\", \" \", comment)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"nltk_nonumbers_tokens\"] = data[\"Comment\"].apply(lambda comment: my_nltk_tokenizer(re.sub(r\"[0-9]+\", \" \", comment)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"nltk_nopunctuationnumbers_tokens\"] = data[\"Comment\"].apply(lambda comment: my_nltk_tokenizer(re.sub(r\"[0-9]+\", \" \", re.sub(r\"[^\\w\\s]\", \" \", comment))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, (estimator, hyper_params) in evaluation_params.items():\n",
    "    for y_name in [\"y8\", \"y6\", \"y2\"]:\n",
    "        scores = {}\n",
    "        for x_name in [\"nltk_nopunctuation_tokens\", \"nltk_nonumbers_tokens\", \"nltk_nopunctuationnumbers_tokens\"]:\n",
    "            score = evaluate(data, x_name, y_name, result_file, vectors_name+\"-\"+x_name, model_name, estimator, hyper_params, embeddings_crawl)\n",
    "            scores[x_name] = score\n",
    "        print(f\"{model_name}-{y_name}-best-{max(scores, key=scores.get)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per programming language analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best: NLTK tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file = \"./results/boe_EN_per_language.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CRAWL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start reading embeddings/crawl-300d-2M.vec.\n",
      "End reading embeddings/crawl-300d-2M.vec.\n"
     ]
    }
   ],
   "source": [
    "embedding_file, vectors_name = (\"embeddings/crawl-300d-2M.vec\", \"crawl_300d_2M\")\n",
    "print(f\"Start reading {embedding_file}.\")\n",
    "embeddings_crawl = KeyedVectors.load_word2vec_format(embedding_file)\n",
    "print(f\"End reading {embedding_file}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WIKI NEWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start reading embeddings/wiki-news-300d-1M.vec.\n",
      "End reading embeddings/wiki-news-300d-1M.vec.\n"
     ]
    }
   ],
   "source": [
    "embedding_file, vectors_name = (\"embeddings/wiki-news-300d-1M.vec\", \"wiki_news_300\")\n",
    "print(f\"Start reading {embedding_file}.\")\n",
    "embeddings_wiki = KeyedVectors.load_word2vec_format(embedding_file)\n",
    "print(f\"End reading {embedding_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "for embeddings, embedding_name in [(embeddings_crawl, \"crawl_300d_2M\"), (embeddings_wiki, \"wiki_news_300\")]:\n",
    "    for lang_name in ['C', 'C++', 'C#', 'Java', 'JavaScript', 'TypeScript', 'PHP', 'Python', 'SQL']:\n",
    "        data_train = data[data.ProgrammingLanguageID != lang_name]\n",
    "        data_test = data[data.ProgrammingLanguageID == lang_name]\n",
    "\n",
    "        for model_name, (estimator, _) in evaluation_params.items():\n",
    "            for y_name in [\"y8\", \"y6\", \"y2\"]:\n",
    "                # Try different cases\n",
    "                for x_name in [\"nltk_tokens\"]:\n",
    "                    X = data_train[x_name]\n",
    "                    y = data_train[y_name]\n",
    "                    # Vectorize.\n",
    "                    X, nan_comments_mask = vectorize(X, embeddings)\n",
    "                    X = X[~nan_comments_mask]\n",
    "                    y = y[~nan_comments_mask]\n",
    "                    \n",
    "                    score_name = make_score_name(f\"{lang_name}-{embedding_name}-{x_name}\", model_name, y.nunique())\n",
    "                    print(\"Evaluation \", score_name)\n",
    "\n",
    "                    pipeline = estimator\n",
    "                    pipeline.fit(X, y)\n",
    "\n",
    "                    X_test = data_test[x_name]\n",
    "                    y_test = data_test[y_name]\n",
    "                    # Vectorize.\n",
    "                    X_test, nan_comments_mask = vectorize(X_test, embeddings)\n",
    "                    X_test = X_test[~nan_comments_mask]\n",
    "                    y_test = y_test[~nan_comments_mask]\n",
    "                    \n",
    "                    y_pred = pipeline.predict(X_test)\n",
    "                    score = f1_score(y_test, y_pred, average=\"macro\")\n",
    "\n",
    "                    pd.DataFrame(\n",
    "                        {\"score_name\": [score_name],\n",
    "                        \"score_value\": [score]}\n",
    "                    ).to_csv(result_file, mode=\"a\", decimal=\",\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
